# -*- coding: utf-8 -*-
"""Task 1 RNN Modeling - Language Modeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJHVcxdMXZVzwqKXU2FrWs8zkoiUV11p

# NLU Task 1 : RNN Modeling

Notebook for ETH NLU course.

By `Benjamin DEVILLERS`, `Adrien BENAMIRA` and `Esteban LANTER`

## Language Modeling
"""

import numpy as np
import tensorflow as tf
from os import path, getcwd

"""### Some general settings"""

max_size = 30  # Max size of the sentences, including  the <bos> and <eos> symbol
vocab_size = 20000 # including symbols
embedding_size = 100  # Size of the embeddig
hidden_size = 512
batch_size = 2  # 64
# Indixes for the <pad>, <bos>, <eos> and <unk> words
pad_index = 0
bos_index = 1
eos_index = 2
unk_index = 3

learning_rate = 0.01

print( 'Tensorflow version '+str(tf.__version__))

"""### Data loading and preprocessing

Let's define the `DataLoader` class. This class will help us to load the dataset and do the corresponding preprocessing.
Preprocessing appens in the `preprocess_data` method.
"""

class DataLoader:
    """
    Dataloader.
    """
  
    def __init__(self, vocab_size=None, max_size=30, tokenizer=None,
                 transform=None):
        """
        :param filename: path of file. If None, will ask to upload a text file.
        :param tokenizer: function to tokenize the sentences. If None, a default
          tokenizer by the space ` ` caracter will be used.
        :param max_size: maximum size of the sentence. All sentences that are longer
          than max_size will be discarded.
        :param vocab_size: Size of the vocab we will use (if default, the vocab will be 
          all the words of the dataset)
        :param transform: some additional transformation(s) to apply to the dataset.
          This can be a callable or a list of collable.
        """
        self.dataset = None
        self.indices = None
        self.max_size = max_size
        self.vocab_size = vocab_size
        self.vocab = None
        self.transform = transform

        if tokenizer is None:
            self.tokenizer = DataLoader.default_tokenizer
        else:
            self.tokenizer = tokenizer
      
    @staticmethod
    def default_tokenizer(sentence):
        """
        Default tokenizer. Split the sentence by the space.
        :param sentence: 
        """
        return sentence.split(' ')
  
    def pad_sentence(self, words):
        """
        Pad the sentence if the length is lower than self.max_size
        :param words: list of words of the sentence
        """
        nb_of_pad = self.max_size - 2 - len(words)
        words = ['<bos>'] + words + ['<eos>']
        words.extend(['<pad>'] * nb_of_pad)
        return words
  
    def set_unk_token(self, words):
        """
        Change all unknown words to the <unk> token
        :param words: list of words of the sentence
        """
        vocab = self.get_vocab()
        return list(map(lambda word: word if word in vocab else '<unk>', words))
        
    def upload_file(self):
        """
        Upload a file to use as dataset.
        The file must be a text file and each input must be separated by the return
        symbol \n.
        """
        # Upload a file to the Notebook
        dataset_file = files.upload()

        # Get the first file from the uploaded list
        dataset = dataset_file[list(dataset_file.keys())[0]].decode("utf-8")

        # Split by return symbol to get all sentences
        self.dataset = dataset.split("\n")

        return self.preprocess_dataset()
    
    def import_file(self, filename):
        """
        Import a file
        """
        dataset_path = path.abspath(path.join(getcwd(), filename))
        # function to remove \n character
        remove_return_character = lambda s: s.replace('\n', '')
        with open(dataset_path, 'r') as f:
            self.dataset = list(map(remove_return_character, f.readlines()))
        return self.preprocess_dataset()
  
    def preprocess_dataset(self):
        """
        Preprocess the dataset.

        * Tokenize the sentences
        * Remove too long sentences
        * Pad the sentences
        * Set the <unk> token for unknown words
        """
        # Tokenize the dataset
        self.dataset = list(map(self.tokenizer, self.dataset))
        # Remove sentences that are too long. 
        # We use self.max_size -2 for <bos> and <eos>
        self.dataset = list(filter(lambda s: len(s) <= (self.max_size - 2), 
                            self.dataset))
        # Set the <unk> words
        self.dataset = list(map(self.set_unk_token, self.dataset))
        # Pad the sentences and add <bos> and <eos>
        self.dataset = list(map(self.pad_sentence, self.dataset))
        # Apply transforms
        if self.transform is not None:
            if type(self.transform) == list:
                for transform in self.transform:
                    self.dataset = transform(self.dataset)
            else:
                self.dataset = self.transform(self.dataset)
        
        # indices of the dataset
        self.indices = list(range(len(self.dataset)))
        self.dataset = np.array(self.dataset)
        np.random.shuffle(self.indices)

        return self  # For method chaining
    
    def get_dataset(self, tf_dataset=False):
        """
        Dataset getter
        :param tf_dataset: if True, will give the tf dataset. Otherwise list dataset.
        """
        return tf.data.Dataset.from_tensors(self.dataset) if tf_dataset else self.dataset
  
    def get_vocab(self):
        """
        Get vocab of the dataset.
        """
        # To be quicker if we have to get the vocab several times
        if self.vocab is not None:
            return self.vocab
        # vocab is a dict where the key is a word and the value the number of ...
        # appearance of the word
        vocab = {}
        for sentence in self.dataset:
            for word in sentence:
                vocab[word] = vocab[word] + 1 if word in vocab.keys() else 1
        # Sort vocab by number of appearance then get the voc
        vocab_ordered = list(zip(*sorted(vocab.items(), key=lambda t: t[1], 
                                         reverse=True)))
        vocab_ordered = vocab_ordered[0]  # Keep only the words
        if self.vocab_size is None:
            self.vocab = vocab_ordered
        else:
            self.vocab = vocab_ordered[:self.vocab_size]
        return self.vocab
  
    def get_word_to_index(self, pad_index=0, bos_index=1, eos_index=2, unk_index=3):
        """
        Build the word to index correspondance
        :param pad_index: index of the padding word
        :param bos_index: index of the bos word
        :param eos_index: index of the eos word
        :param unk_index: index of the unk word
        :rtype: tuple(dict, dict)
        :return: couple of word to index correspondance and index to word correspondance.
        """
        vocab = self.get_vocab()
        word_to_index = {
            "<pad>": pad_index,
            "<bos>": bos_index,
            "<eos>": eos_index,
            "<unk>": unk_index
        }
        index_to_word = {
            pad_index: "<pad>",
            bos_index: "<bos>",
            eos_index: "<eos>",
            unk_index: "<unk>"
        }
        max_already_taken_token = max([pad_index, bos_index, eos_index, unk_index])
        for k, word in enumerate(vocab):
            word_to_index[word] = max_already_taken_token + k + 1
            index_to_word[max_already_taken_token + k + 1] = word
        return word_to_index, index_to_word
    
    def apply_transformation(self, transform):
        """
        Apply a transformation to the current dataset
        :param transform: function to apply to the dataset.
            The transform function gets a value of the dataset and has to return the new value.
        """
        self.dataset = list(map(transform, self.dataset))
        return self
    
    def regenerate_indices(self):
        """
        Rebuilds the list of shuffled dataset indices for random batching
        """
        self.indices = list(range(len(self.dataset)))
        np.random.shuffle(self.indices)
    
    def get_batch(self, batch_size):
        """
        Get a batch of random elements
        """
        # If there is not enough elements, restart from scratch
        if len(self.indices) < batch_size:
            self.regenerate_incices()
        return np.array([self.dataset[self.indices.pop()] for k in range(batch_size)])

"""Upload a dataset.

**Remark.** Apparently the `upload_file` method only works on the chrome/chromium browser. :-(
"""


dataloader_eval = DataLoader(vocab_size, max_size).import_file('dataset/sentences.train')

"""Let's do some test on the dataloader..."""

# Get the word to index correspondance for the embedding.
word_to_index, index_to_word = dataloader_eval.get_word_to_index(pad_index, bos_index,
                                                            eos_index, unk_index)
print("The index of 'the' is:", word_to_index["the"])
print("The word of index 100 is:", index_to_word[100])

def word_to_index_transform(word_to_index, data):
    """
    Get a batch of sequences and transform the words into indices
    :param word_to_index: dict of relations between words and indices
    :param data: batch to apply the transformation on.
    """
    def transform_word(word):
        assert word in word_to_index.keys(), "The word {} is not in the vocab".format(word)
        return word_to_index[word]
    return np.array(list(map(lambda sequence: list(map(transform_word, sequence)), data)))

"""### The LSTM Model

New code, using scan because while_loop is not really required (fixed number of loops). I removed the TensorArray too
"""

class LSTM:
    
    def __init__(self, batch_size, embedding_size, vocab_size, hidden_size, max_size, teacher_forcing=True):
        self.batch_size = batch_size
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.max_size = max_size
        self.teacher_forcing = teacher_forcing
        
        
    def __call__(self, x, label=None):
        assert self.teacher_forcing and label is not None, "Please provide the label to use teacher forcing."
        
        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(self.hidden_size)

        # We give in the vector [bos, tag1, tag2, ..., last tag]
        # and expect to receive [tag1, tag2, ..., sos]
        with tf.variable_scope("embedding", reuse=tf.AUTO_REUSE):
            word_embeddings = tf.get_variable("word_embeddings", 
                                              [self.vocab_size, self.embedding_size], dtype=tf.int32)

        inputs = tf.nn.embedding_lookup(word_embeddings, x)
        if label is not None:
            embedded_labels = tf.nn.embedding_lookup(word_embeddings, label)


        # Sets the size into time x batch x embedding
        x_t = tf.transpose(inputs, [1, 0, 2], name="x_embedded_transposed")
        
        if label is not None:
            label_t = tf.transpose(embedded_labels, [1, 0, 2], name="y_embedded_transposed")


        # weight for the softmax
        with tf.variable_scope("softmax", reuse=tf.AUTO_REUSE):
            W = tf.get_variable("W", shape=(self.max_size-1, self.batch_size, 
                                            self.vocab_size, self.hidden_size), 
                                initializer=tf.contrib.layers.xavier_initializer())

        default_state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros([self.batch_size, self.hidden_size], 
                                                               name="state1"), 
                                                      tf.zeros([self.batch_size, self.hidden_size], 
                                                               name="state2"))

        def body(init, vect):
            state, k = init
            # Get the word of time step k
            # Teacher forcing
            if not self.teacher_forcing or k == 0:
                in_vect = vect  # put the last generated vector in
            else:
                in_vect = label_t[k-1]  # Put the last label in for faster training
            in_vect = tf.cast(in_vect, tf.float32)
            _, state = rnn_cell(in_vect, state)
            return (state, k+1)

        # Counter to count the number of words (max max_size)
        k = 0

        state, _ = tf.scan(body, x_t, (default_state, k))
        output = state.h
        print(state)
        # Add a dimension for being able to multiply with W
        final_output = tf.expand_dims(output, 3)
        final_output = tf.matmul(W, final_output) # Premier
        print(final_output.shape)
        final_output = tf.squeeze(final_output, 3)
        final_output = tf.transpose(final_output, [1, 0, 2])
        return final_output, tf.nn.softmax(final_output)
    
    
    def optimize(self, x, label, output, learning_rate):
        training_vars = tf.trainable_variables()
        # En fait je ne suis pas sur, j'ai l'impression que la fonction sparse_softmax_cross_entropy_with_logits calcul un softmax
        # vu son nom. En plus il prend les `logits`... donc effectivement, il ne faut pas faire de softmax avant...
        # Le problème c'est pour l'évaluation... Vu que cette fonction n'est utilisée que pour l'entraînement.
        # Il faudrait donc vérifier en fonction de si `label is None` ou pas pour savoir si on entraîne ou si on
        # évalu... Si on évalu, il faudrait garde le premier softmax
        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)) # Deuxième...

        grads, _ = tf.clip_by_global_norm(tf.gradients(cross_entropy, training_vars), 5)  # Max gradient of 5

        optimizer = tf.train.AdamOptimizer(learning_rate)
        optimizer.apply_gradients(zip(grads, training_vars))

        return optimizer.minimize(cross_entropy)

lstm = LSTM(batch_size, embedding_size, vocab_size, hidden_size, max_size)

x = tf.placeholder(tf.int32, (batch_size, max_size-1), name="x")
label = tf.placeholder(tf.int32, (batch_size, max_size-1), name="label")

output, softmax_output = lstm(x, label)

"""We now have to define the optimization part."""

with tf.variable_scope("optimizer", reuse=tf.AUTO_REUSE):
    optimizer = lstm.optimize(x, label, output, learning_rate)

"""Now let's execute the graph in the session.

We ge a data batch with `dataloader.get_batch(batch_size)`. This fetches a batch of word sequences.

We then need to transform that into a batch of word index. We can achieve this with the helper function
`word_to_index_transform(word_to_index, word_batch)` defined before.

furthermore, we need to seperate the batch into the input batch and the target batch.
We will do that by separating the `max_size - 1` first index of the sequences into the input sequences and
assign the `max_size - 1` last tokens into the target sequences.
"""

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # Get a batch with the dataloader and transfrom it into tokens
    batch = word_to_index_transform(word_to_index, dataloader_eval.get_batch(batch_size))
    # Defining input and target sequences
    batch_input, batch_target = batch[:, :-1], batch[:, 1:]
    # Run the session
    result = sess.run(softmax_output, {x: batch_input, label: batch_target})

# """Train"""

# with tf.Session() as sess:
    # sess.run(tf.global_variables_initializer())
    # for epoch in range(100000):
        # # Get a batch with the dataloader and transfrom it into tokens
        # batch = word_to_index_transform(word_to_index, dataloader_eval.get_batch(batch_size))
        # # Defining input and target sequences
        # batch_input, batch_target = batch[:, :-1], batch[:, 1:]
        # # Run the session
        # sess.run(optimizer, {x: batch_input, label: batch_target})

