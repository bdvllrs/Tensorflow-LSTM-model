# -*- coding: utf-8 -*-
"""Task 1 RNN Modeling - Language Modeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJHVcxdMXZVzwqKXU2FrWs8zkoiUV11p

# NLU Task 1 : RNN Modeling

Notebook for ETH NLU course.

By `Benjamin DEVILLERS`, `Adrien BENAMIRA` and `Esteban LANTER`

## Language Modeling
"""

import tensorflow as tf
from utils import DataLoader, word_to_index_transform
from LSTM import LSTM

"""### Some general settings"""

max_size = 30  # Max size of the sentences, including  the <bos> and <eos> symbol
vocab_size = 20000  # including symbols
embedding_size = 100  # Size of the embeddig
hidden_size = 512
batch_size = 2  # 64
# Indixes for the <pad>, <bos>, <eos> and <unk> words
pad_index = 0
bos_index = 1
eos_index = 2
unk_index = 3

learning_rate = 0.01

dataloader_eval = DataLoader(vocab_size, max_size).import_file('dataset/sentences.train')

"""Let's do some test on the dataloader..."""

# Get the word to index correspondance for the embedding.
word_to_index, index_to_word = dataloader_eval.get_word_to_index(pad_index, bos_index,
                                                                 eos_index, unk_index)
print("The index of 'the' is:", word_to_index["the"])
print("The word of index 100 is:", index_to_word[100])

lstm = LSTM(batch_size, embedding_size, vocab_size, hidden_size, max_size)

x = tf.placeholder(tf.int32, (batch_size, max_size - 1), name="x")
label = tf.placeholder(tf.int32, (batch_size, max_size - 1), name="label")

output, softmax_output = lstm(x, label)

"""We now have to define the optimization part."""

with tf.variable_scope("optimizer", reuse=tf.AUTO_REUSE):
    optimizer = lstm.optimize(output, label, learning_rate)

"""Now let's execute the graph in the session.

We ge a data batch with `dataloader.get_batch(batch_size)`. This fetches a batch of word sequences.

We then need to transform that into a batch of word index. We can achieve this with the helper function
`word_to_index_transform(word_to_index, word_batch)` defined before.

furthermore, we need to seperate the batch into the input batch and the target batch.
We will do that by separating the `max_size - 1` first index of the sequences into the input sequences and
assign the `max_size - 1` last tokens into the target sequences.
"""

# with tf.Session() as sess:
#     sess.run(tf.global_variables_initializer())
#     # Get a batch with the dataloader and transfrom it into tokens
#     batch = word_to_index_transform(word_to_index, dataloader_eval.get_batch(batch_size))
#     # Defining input and target sequences
#     batch_input, batch_target = batch[:, :-1], batch[:, 1:]
#     # Run the session
#     result = sess.run(softmax_output, {x: batch_input, label: batch_target})

# """Train"""

# with tf.Session() as sess:
# sess.run(tf.global_variables_initializer())
# for epoch in range(100000):
# # Get a batch with the dataloader and transfrom it into tokens
# batch = word_to_index_transform(word_to_index, dataloader_eval.get_batch(batch_size))
# # Defining input and target sequences
# batch_input, batch_target = batch[:, :-1], batch[:, 1:]
# # Run the session
# sess.run(optimizer, {x: batch_input, label: batch_target})
